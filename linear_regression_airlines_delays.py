# -*- coding: utf-8 -*-
"""Linear_regression_airlines_delays.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dRl5Gi-cjLlpbyLIOzeRWPViFxWTBvWb

Download necessary libraries.
"""

# Import necessary libraries
import pandas as pd
import numpy as np


import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import PolynomialFeatures
import warnings
warnings.filterwarnings('ignore')


# Set display options
pd.set_option('display.max_columns', None)
plt.style.use('default')

"""Load the dataset and create a dataframe."""

df = pd.read_csv(r"C:\Users\benne\Downloads\archive (1)\Airlines_delays.csv")
df.head()

df.shape

"""Explore the dataset."""

print('Dataset Description')
df.info()

df.describe()

print('Missing values per column:')
df.isnull().sum()

"""Dropping the 'id' and 'Flight' columns as they are identifiers and not useful for predicting delays."""

# Drop unnecessary columns
df = df.drop(['id', 'Flight'], axis=1)
print(df.head())

# Explore unique values and their counts for 'Airline'
print("Unique values and counts for 'Airline':")
print(df['Airline'].value_counts())

# Explore unique values and their counts for 'AirportFrom'
print("\nUnique values and counts for 'AirportFrom':")
print(df['AirportFrom'].value_counts())

# Explore unique values and their counts for 'AirportTo'
print("\nUnique values and counts for 'AirportTo':")
print(df['AirportTo'].value_counts())

"""Exploratory Data Analysis and Visualization."""

# Calculate the delay rate per airline
airline_delay_rate = df.groupby('Airline')['Delay'].mean().sort_values(ascending=False)

# Create a bar plot
plt.figure(figsize=(12, 6))
sns.barplot(x=airline_delay_rate.index, y=airline_delay_rate.values, palette='viridis')
plt.title('Flight Delay Rate by Airline')
plt.xlabel('Airline')
plt.ylabel('Delay Rate')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Calculate total traffic for each airport (arrivals + departures)
airport_traffic = pd.concat([df['AirportFrom'], df['AirportTo']]).value_counts()

# Identify the top 10 busiest airports
top_airports = airport_traffic.head(10).index.tolist()

print("Top 10 Busiest Airports:")
print(top_airports)

# Filter the DataFrame to include only flights involving the top airports
top_airports_df = df[df['AirportFrom'].isin(top_airports) | df['AirportTo'].isin(top_airports)]

# Calculate the delay rate for flights *from* each top airport
airport_from_delay_rate = top_airports_df.groupby('AirportFrom')['Delay'].mean()

# Calculate the delay rate for flights *to* each top airport
airport_to_delay_rate = top_airports_df.groupby('AirportTo')['Delay'].mean()

# Combine the delay rates (using mean if an airport is both 'from' and 'to' in the filtered data)
# We'll only consider the rates for the airports in our top_airports list
airport_delay_rates = pd.concat([airport_from_delay_rate, airport_to_delay_rate]).groupby(level=0).mean()

# Filter to keep only the rates for the actual top 10 airports identified earlier
airport_delay_rates = airport_delay_rates[airport_delay_rates.index.isin(top_airports)].sort_values(ascending=False)

print("Delay Rate for Top 10 Busiest Airports:")
print(airport_delay_rates)

# Create a bar plot for airport delay rates
plt.figure(figsize=(14, 7))
sns.barplot(x=airport_delay_rates.index, y=airport_delay_rates.values, palette='viridis')
plt.title('Flight Delay Rate for Top 10 Busiest Airports')
plt.xlabel('Airport')
plt.ylabel('Average Delay Rate')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Completing correlation matrix for numerical columns before one-hot coding due to how large the dataset is."""

# Select numerical columns for correlation matrix
numerical_cols = ['DayOfWeek', 'Time', 'Length', 'Delay']
numerical_df = df[numerical_cols]

# Calculate the correlation matrix
correlation_matrix = numerical_df.corr()

# Display the correlation matrix
print("Correlation Matrix for Numerical Columns:")
print(correlation_matrix)

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

print('Correlations with Delay (sorted by absolute value)')
price_correlations = correlation_matrix['Delay'].drop('Delay').sort_values(key = abs, ascending=False)
price_correlations

"""One-Hot coding categorical columns into numbers."""

# One-hot encode categorical variables
categorical_cols = ['Airline', 'AirportFrom', 'AirportTo']
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
print(df.head())

"""Separate features and target."""

# Separate Features from the Target
X = df.drop('Delay', axis=1)
y = df['Delay']

y

# features shape
print(f'Features shape: {X.shape}')
print(f'Target shape: {y.shape}')

# get feature names
print(f'Feature names: {list(X.columns)}')

"""Splitting data into training and testing."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# take a look at the shapes of the training and testing sets
print(f'\n Training set shape {X_train.shape}')
print(f'Test set shape {X_test.shape}')
print(f'Training target set shape {y_train.shape}')
print(f'Testing target set shape {y_test.shape}')

X_train

# Feature scaling using StandardScaler
print('Before scaling - Training set statistics')
print(X_train.describe())

# Initialize the scaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

type(X_train_scaled)

# Convert back to DataFrame for easier handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns, index = X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns = X_test.columns, index = X_test.index)

print('After Scaler - Training set statistics')
print(X_train_scaled.describe())

multiple_model = LinearRegression()

# training the model
multiple_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_train = multiple_model.predict(X_train_scaled)

y_pred_test = multiple_model.predict(X_test_scaled)

# calculate performance metrics for both training and testing sets
def calculate_metrics(y_true, y_pred, dataset):
  mse = mean_squared_error(y_true, y_pred)
  rmse = np.sqrt(mse)
  mae = mean_absolute_error(y_true, y_pred)
  r2 = r2_score(y_true, y_pred)

  print(f'Performance Metrics: {dataset}')
  print(f"Mean Squared Error (MSE): {mse:.4f}")
  print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
  print(f"Mean Absolute Error (MAE): {mae:.4f}")
  print(f"R-squared (RÂ²) Score: {r2:.4f}")

# Training test performance
calculate_metrics(y_train, y_pred_train, 'Training')

# Testing Data Performance
calculate_metrics(y_test, y_pred_test, 'Testing')

# Create and train the classifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)
